{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2616,
     "status": "ok",
     "timestamp": 1746001869027,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "sBbpJICfyu-P"
   },
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Packages\n",
    "\n",
    "!pip install -q \\\n",
    "    accelerate==0.29.2 \\\n",
    "    aiohttp==3.9.5 \\\n",
    "    aiosignal==1.3.1 \\\n",
    "    async-timeout==4.0.3 \\\n",
    "    attrs==23.2.0 \\\n",
    "    bitsandbytes==0.43.1 \\\n",
    "    certifi==2024.2.2 \\\n",
    "    charset-normalizer==3.3.2 \\\n",
    "    click==8.1.7 \\\n",
    "    colorama==0.4.6 \\\n",
    "    datasets==2.18.0 \\\n",
    "    dill==0.3.8 \\\n",
    "    filelock==3.13.4 \\\n",
    "    frozenlist==1.4.1 \\\n",
    "    fsspec==2024.2.0 \\\n",
    "    huggingface-hub==0.22.2 \\\n",
    "    idna==3.7 \\\n",
    "    jinja2==3.1.3 \\\n",
    "    joblib==1.4.0 \\\n",
    "    markupsafe==2.1.5 \\\n",
    "    mpmath==1.3.0 \\\n",
    "    multidict==6.0.5 \\\n",
    "    multiprocess==0.70.16 \\\n",
    "    networkx==3.2.1 \\\n",
    "    nltk==3.8.1 \\\n",
    "    numpy==1.26.4 \\\n",
    "    packaging==24.0 \\\n",
    "    pandas==2.2.2 \\\n",
    "    peft==0.10.0 \\\n",
    "    psutil==5.9.8 \\\n",
    "    pyarrow==15.0.2 \\\n",
    "    pyarrow-hotfix==0.6 \\\n",
    "    python-dateutil==2.9.0.post0 \\\n",
    "    pytz==2024.1 \\\n",
    "    pyyaml==6.0.1 \\\n",
    "    regex==2023.12.25 \\\n",
    "    requests==2.31.0 \\\n",
    "    safetensors==0.4.3 \\\n",
    "    six==1.16.0 \\\n",
    "    sympy==1.12 \\\n",
    "    tokenizers==0.15.2 \\\n",
    "    torch==2.2.2 \\\n",
    "    tqdm==4.66.2 \\\n",
    "    transformers==4.39.3 \\\n",
    "    typing-extensions==4.11.0 \\\n",
    "    urllib3==2.2.1 \\\n",
    "    xxhash==3.4.1 \\\n",
    "    yarl==1.9.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2209,
     "status": "ok",
     "timestamp": 1746001871237,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "fXl4_rmWz-zy",
    "outputId": "03604790-abfb-4cf3-fcb1-68d5bf2ad237"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSauf75EqzgO"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331,
     "referenced_widgets": [
      "7044f4a0073946fbbfc18b05ce7b1d70",
      "6a0057b1a71641bd94d1fe5c2e7542b5",
      "f42628f3762d48e1a6eb086180453d6f",
      "e852bb96d22c43928b6479a623e62968",
      "5b33b9d4f11a48a4850790472bfd8841",
      "d5c8baf5d63745d2b7821353c7338e3e",
      "274e82b9a7094df4b6bb4bde89915535",
      "1d1cd900f3e6445f95333d1842552d3d",
      "c930b8d3aa924af9b800633599377c03",
      "b2133374bf0c415fa7dbda8da891eb78",
      "893f53981bfe4f12a69e60777bc16e78",
      "73c6ff136cfc480fad567cf126b31a6e",
      "902a4a861c4f405083f404b97838d9b8",
      "6e9d1e14743b4be7b7f5ebb9d3be84ff",
      "29964d92fb454154a764483ee9dbe20a",
      "d81508528b354265984147b7ab4d727b",
      "471b2cf3475c4529b9c445979aaef8d6"
     ]
    },
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1746001871425,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "Fc-F7lu9qV1G",
    "outputId": "0f106d68-0c75-4527-fc13-f3cb8a95db3e"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1307,
     "status": "ok",
     "timestamp": 1746001872733,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "IoUNdBRFqYvx",
    "outputId": "8d5c233a-c1b5-4dff-a6c4-d0b9c82e7d74"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1746001873010,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "JAC-byzR0PsE",
    "outputId": "26ce9a5d-54f3-4c93-84ae-c81a6090c03e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtYeShp9sjEq"
   },
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5204,
     "status": "ok",
     "timestamp": 1746001878216,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "6pS52m0Hr-Wc"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import bitsandbytes as bnb\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed,  BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments, LlamaTokenizer, EarlyStoppingCallback\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUcKB00Yspk_"
   },
   "source": [
    "Tokenizer and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746001878220,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "RtIDrAsWssD9"
   },
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "def load_model(model_name, bnb_config):\n",
    "    print(\"N GPUS:\", torch.cuda.device_count())\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BT-ztcdswdt"
   },
   "source": [
    "Prompt Format and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1746001878228,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "2dUe0LLGszMZ"
   },
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    instruction = \"What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\"\n",
    "    sample[\"text\"] = f\"<s>[INST] {instruction}\\\\n\\\\n{sample['Sentence']} [/INST] {sample['Keywords']} </s>\"\n",
    "    return sample\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    return tokenizer(batch[\"text\"], max_length=max_length, truncation=True)\n",
    "\n",
    "def preprocess_dataset(tokenizer, max_length, seed, dataset):\n",
    "    dataset = dataset.map(create_prompt_formats)\n",
    "    print(\"Sample prompt:\\\\n\", dataset[0][\"text\"])\n",
    "    f = partial(preprocess_batch, tokenizer=tokenizer, max_length=max_length)\n",
    "    dataset = dataset.map(f, batched=True)\n",
    "    dataset = dataset.filter(lambda x: len(x[\"input_ids\"]) < max_length)\n",
    "    return dataset.shuffle(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCKrOEnes2Ph"
   },
   "source": [
    "LoRA Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1746001878229,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "3h82l-Ovs5B4"
   },
   "outputs": [],
   "source": [
    "def create_peft_config(target_modules):\n",
    "    return LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=0.0,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    return list({name.split(\".\")[-1] for name, mod in model.named_modules() if isinstance(mod, cls)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PZKHQkFs8Bx"
   },
   "source": [
    "Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746001878237,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "geNQYmbYs9eA"
   },
   "outputs": [],
   "source": [
    "def train(model, tokenizer, train_dataset, val_dataset, output_dir):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    modules = find_all_linear_names(model)\n",
    "    peft_config = create_peft_config(modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable params: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=1,\n",
    "            num_train_epochs=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=5,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",  # ‚úÖ BU satƒ±r gerekli\n",
    "            report_to=None\n",
    "    ),\n",
    "\n",
    "\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCPw4vP_tGYP"
   },
   "source": [
    "Load Dataset and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "54f96a6b0b8e4c50981c4a47abe382cd",
      "87f1aca4884c4fdaa78b51c13e0c0bb8",
      "8a06cae20b5e40368d9241cdf9d7c5c4",
      "91452882510f417dae02606389c8dff9",
      "2c10ca06cc8343e7b6dce05c66768c88",
      "0646e606b2c04149ae072580cfca25ba",
      "2e8d9b78a0a9416cbd3b2281fa2509d0",
      "e8899b1849aa4a9ea10b6769e5b08c90",
      "85b56b47c22147ddbe7b2387314236d6",
      "25dc071ca06241dbb193c83cff98979b",
      "0fb808e6bb9c45ab84e61a24792bced4",
      "e178b462d6b34511897d1a9307d69c56",
      "c77241cfffd7470fb94a2978d8e8258f",
      "3823f01208694d059a946e86e224e531",
      "8dceaf4080274be38d2a8feb181f8b73",
      "69f5aeb19c684005821db24c880a3708",
      "6ff392bef087470d802e2cea3e7353b0",
      "2e5d61e9472446f1a31ea4a168780e4c",
      "43ef79d6dd0e41c7b9dbdcbbda96dcfa",
      "f7529fc5584940d1be6fbd50f69b3e35",
      "57cc12523ff4404db600753f49e0aed2",
      "f3a6dd10323d4120bf735940881b8fb0",
      "c681b33b9c6b4cf48241059f2d91a867",
      "1444483a80224f0b86037e973f19c2c9",
      "5f2d8c21c3ad4ab59375b65e991b2633",
      "5081c14a941b437e8d91a298d7048b44",
      "4749620477c4458cb0d7b2859790d163",
      "0bc5e5d36a184efc887a50b0f69c8c68",
      "11ea1cf190aa4ad4bd7c5797d65799fd",
      "2a7957b4c28149c7be2fd73b4832cb92",
      "027c2e2c594d43fbb32e1a2c2b07a2dc",
      "3f241372fa264da6b1d8281090cd28bd",
      "1e45f093f097467f920a3376d021a25a",
      "af32df1f993a43d4bd0be3bc42fed755",
      "9d938f0cf80141a99458dcc8025f5991",
      "10e043f117ea4868924e1661eaba8ede",
      "d98653d81769461abc1723ae4a407f5c",
      "ad8e42865f8e4c3facf9992dbef7185d",
      "2713810785ec4d6dae9e291c2d8b6212",
      "28db11af7445478787f39e37845b2b61",
      "e330374f5ae14325ac4758a9421338d2",
      "5b170a3cb0a4431b9a9efdf547f343e7",
      "67cfc4c2a4d046eaa758a4f78a019067",
      "5e9ec5748936442bbf57d28c4a153772",
      "823a59f1121b4d9193b18576f1b6b320",
      "f3e818eb80834088915aec1d0aafdc45",
      "18de4bc270be460c9ad7e97761483d82",
      "5be1554f92864db3b443f7d02e162f36",
      "b3d3c0f88b124003a2dfbb996360b1b0",
      "813fe8b2ce11440090d752b7891899f7",
      "c0eeb37bb04d43cca15c327337b6413a",
      "f3f0c846257940019fc78901964e3a21",
      "a53600e63c9d4a19acd2959cf4096630",
      "66bad995dc644e228b4ab30298a3f7ab",
      "4e718bc3d4d244a3884c2f30c426a8d6",
      "d252a4671c2f449b8b20cfcd091bb9de",
      "d114806b011f4c4494b8edf21356499f",
      "d81fa2d933fb453ea8b17ac39d9b0c05",
      "067c428c1a6e4f678346248d11b75359",
      "76e4b94f5f904326b3c91d03ca805bdb",
      "0c00f889174c4f77a14cad6d380a94a8",
      "db64a36d5be44946b6a59db20922352e",
      "8d869fca99fb4c7298565934b0d836ac",
      "1bd6e137483646499d969a817b99ca5d",
      "95cc96eb32a84025ae04b543ae704fb0",
      "01824d7949f44ffe85401240e501ce88",
      "96afdc4be9ec4cdeaf567efe8ca3d3fe",
      "f3b5b36d44bb4787a3c03090b8ed6bb4",
      "42c97ca5d8d546eaab1489955a51819e",
      "72df15dff91a4bf1bcd7a1df7d0b4a9e",
      "287573517fec49258583f4c412587e0c",
      "8f1c77b01b3046be887e91d0a9b2cf7c",
      "773f941c54d94ee1b5a3d263ca9a874f",
      "26911666c0754007bd8f3ce5b1f824fe",
      "0ecd90cec9fe4f91bd5090be1852919f",
      "4c6024932d044c3b901c902b1ee78bde",
      "f850541437ff4297ab8eb0d9fb841070"
     ]
    },
    "executionInfo": {
     "elapsed": 383256,
     "status": "ok",
     "timestamp": 1746002261493,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "6bagZAv9tEDP",
    "outputId": "df5b3905-a418-45e0-cb43-1a5f0f91fac0"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = load_dataset(\"bengisucam/LLL_INO-tagged\", split=\"train\")\n",
    "train_test_dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_dataset[\"train\"]\n",
    "val_dataset = train_test_dataset[\"test\"]\n",
    "\n",
    "# Model ve tokenizer\n",
    "model_name = \"meta-llama/Llama-2-13b-chat-hf\"  # \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "bnb_config = create_bnb_config()\n",
    "model, tokenizer = load_model(model_name, bnb_config)\n",
    "\n",
    "# Preprocessing\n",
    "max_length = 1024\n",
    "train_dataset_processed = preprocess_dataset(tokenizer, max_length, seed, train_dataset)\n",
    "val_dataset_processed = preprocess_dataset(tokenizer, max_length, seed, val_dataset)\n",
    "\n",
    "# Train\n",
    "output_dir = \"results/llama2_colab/\"\n",
    "train(model, tokenizer, train_dataset_processed, val_dataset_processed, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo4WyV8FtDwP"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wD8OyOgf21hZ"
   },
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1746002640507,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "MFpG6W6Q3e7L"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import  LlamaTokenizer, set_seed\n",
    "from peft import  AutoPeftModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime as dt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1746002643522,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "wK6hR97V22P4"
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "def create_prompt_formats_for_test(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction', 'context', 'response')\n",
    "    Then concatenate them using two newline characters\n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    # Instruction Key without protein tags:\n",
    "    # INSTRUCTION_KEY = \"### Instruction: What is the key word that represents the interaction between the proteins \" + sample[\"Gene1\"] + \" and \" + sample[\"Gene2\"] + \" in the given sentence?\"\n",
    "\n",
    "    # Instruction Key with protein tags:\n",
    "    INSTRUCTION_KEY = \"### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\"\n",
    "    INPUT_KEY = \"### Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "\n",
    "\n",
    "    blurb = f\"{INTRO_BLURB}\"\n",
    "    instruction = INSTRUCTION_KEY\n",
    "    input_context = f\"{INPUT_KEY}\\n{sample['Sentence']}\"   # Sentence, passage\n",
    "    response = f\"{RESPONSE_KEY}\\n\"\n",
    "\n",
    "\n",
    "    parts = [part for part in [blurb, instruction, input_context, response] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ece5ba7edf5148aab53090d71541d0ab",
      "b23c5c54b8a14dbfb15f3cfd28b47588",
      "fd855201978e4a4bb0aa6b09ada2a751",
      "47d97e7422b544d59ad1686557c6e87a",
      "9c273669e4544b30a3426a2be75c29bc",
      "18aab58cdfba4f52b019cdc4b6205ff2",
      "318d43448983419d87eb8e640af99c79",
      "b26b6c67322f43f69dd80ce1586f3245",
      "baf746d7800745bcb6465673c185ff87",
      "cef67ece396f4b929d032985c19dbe8b",
      "647859d099d243c59096654f5e7121d9",
      "c4605ac54f4f4b4789991d4ed46d3701",
      "8ba80efe67844215bbb6e1ec7439f0c4",
      "4dfdf1b71bd04fb68a82ca81e3110dbc",
      "057bf9c740074ab28652ec637df12576",
      "b98f999687764b85aa2288ae0c0244b4",
      "f5ba1f54a863401d82d1d7c8ba72c94e",
      "ed39ca3b698847899c5bea8f45324a5b",
      "5b5f703ad9bd4ebeb2b94bb72d7a52ff",
      "dfef3e701cb74f969532be19e8f6a427",
      "83729a98387641879c2be54b47477d66",
      "a85c967ba4e34de0807dd3f4e73b4807"
     ]
    },
    "executionInfo": {
     "elapsed": 104907,
     "status": "ok",
     "timestamp": 1746002831939,
     "user": {
      "displayName": "B√º≈üra Oƒüuzoƒülu",
      "userId": "17518526586539935017"
     },
     "user_tz": -180
    },
    "id": "sgpHYenW28kn",
    "outputId": "4679a4c2-af6d-402c-abbc-9f693219a051"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"finetune_results/finetuned-7B-chat-test-5.log\", level=logging.INFO)\n",
    "logging.info(f\"({dt.now().strftime('%d/%m/%Y %H:%M:%S')})| START\")\n",
    "\n",
    "\n",
    "test_on_lll=True\n",
    "\n",
    "# Specify device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## TEST Finetuned Model From Checkpoint ##\n",
    "tmp_model_path = \"results/llama2_colab\"\n",
    "print(\"Loading the checkpoint in a Llama model.\")\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True).to(device)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(tmp_model_path, use_fast=False)\n",
    "\n",
    "## check the total model parameters\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "if test_on_lll:\n",
    "    test_dataset = load_dataset(\"bengisucam/LLL_INO-tagged\", split=\"test\")\n",
    "else:\n",
    "    test_dataset = load_dataset(\"bengisucam/HPRD50_true_only_tagged\", split=\"test\")\n",
    "    print(test_dataset[:2])\n",
    "    test_dataset = test_dataset.filter(lambda example: example[\"isValid\"]==True)\n",
    "    print(test_dataset[:2])\n",
    "\n",
    "print(len(test_dataset))\n",
    "# Add prompt to each sample\n",
    "print(\"Preprocessing dataset...\")\n",
    "dataset = test_dataset.map(create_prompt_formats_for_test)  # , batched=True)\n",
    "print(len(dataset))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "   # Specify input\n",
    "    text = dataset[i][\"text\"]\n",
    "    sentence_id = dataset[i][\"Unnamed: 0\"]\n",
    "\n",
    "\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Get answer\n",
    "    # (Adjust max_new_tokens variable as you wish (maximum number of tokens the model can generate to answer the input))  #.to(device)\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"],\n",
    "                             max_new_tokens=50, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    print(\"EXAMPLE \", i+1)\n",
    "    # Decode output & print it\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Sentence Id: \", sentence_id)\n",
    "    print(response)\n",
    "    print(\"##############################################################################\")\n",
    "    logging.info(\"Sentence Id: %s, Response: %s  .\\n\\n\", sentence_id, response)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMgh5NJA3c32Q3UrbUN5caI",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "state": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
