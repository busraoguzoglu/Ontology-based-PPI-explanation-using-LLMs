{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_name = mistralai/Mistral-7B-Instruct-v0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (1.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bitsandbytes-0.46.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate peft bitsandbytes transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb3f37744a24fdca5675111ce255026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.0.2\n",
      "Pandas version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.52.4\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /usr/local/lib/python3.11/dist-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, sentence-transformers\n",
      "---\n",
      "Name: accelerate\n",
      "Version: 1.7.0\n",
      "Summary: Accelerate\n",
      "Home-page: https://github.com/huggingface/accelerate\n",
      "Author: The HuggingFace team\n",
      "Author-email: zach.mueller@huggingface.co\n",
      "License: Apache\n",
      "Location: /usr/local/lib/python3.11/dist-packages\n",
      "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
      "Required-by: peft\n",
      "---\n",
      "Name: peft\n",
      "Version: 0.15.2\n",
      "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
      "Home-page: https://github.com/huggingface/peft\n",
      "Author: The HuggingFace team\n",
      "Author-email: benjamin@huggingface.co\n",
      "License: Apache\n",
      "Location: /usr/local/lib/python3.11/dist-packages\n",
      "Requires: accelerate, huggingface_hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "pip show transformers accelerate peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import bitsandbytes as bnb\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed,  BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments, LlamaTokenizer, EarlyStoppingCallback\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Loader and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    print(\"N GPUS:\", torch.cuda.device_count())\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True  # Qwen için mutlaka True\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True  # Qwen için mutlaka True\n",
    "    )\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Format and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama2 prompt format: [INST] {instruction}\\n\\n{sentence} [/INST] {answer}\n",
    "Llama3 prompt format: <|begin_of_text|><|user|>\n",
    "{instruction_and_input}\n",
    "<|assistant|>\n",
    "{expected_output}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Qwen Prompt format (may need to double check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    instruction = \"What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\"\n",
    "    user_prompt = f\"{instruction}\\n\\n{sample['Sentence']}\"\n",
    "    answer = sample['Keywords']\n",
    "\n",
    "    sample[\"text\"] = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": user_prompt}, {\"role\": \"assistant\", \"content\": answer}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    return tokenizer(batch[\"text\"], max_length=max_length, truncation=True)\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def preprocess_dataset(tokenizer, max_length, seed, dataset):\n",
    "    dataset = dataset.map(create_prompt_formats, batched=False)  # ✅ Burada batched=False\n",
    "    print(\"Sample prompt:\\n\", dataset[0][\"text\"])\n",
    "    f = partial(preprocess_batch, tokenizer=tokenizer, max_length=max_length)\n",
    "    dataset = dataset.map(f, batched=True)\n",
    "    dataset = dataset.filter(lambda x: len(x[\"input_ids\"]) < max_length)\n",
    "    return dataset.shuffle(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(target_modules):\n",
    "    return LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"w1\", \"w2\", \"w3\", \"out_proj\", \"lm_head\"], # target modules for qwen are different\n",
    "        lora_dropout=0.0,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    return list({name.split(\".\")[-1] for name, mod in model.named_modules() if isinstance(mod, cls)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized Train Function for Qwen and A100 Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.base_layer.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.0.self_attn.q_proj.base_layer.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.q_proj.base_layer.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.0.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.0.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.0.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.0.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.0.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.0.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.1.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.1.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.1.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.1.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.1.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.1.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.1.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.2.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.2.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.2.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.2.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.2.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.2.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.2.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.3.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.3.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.3.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.3.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.3.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.3.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.3.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.4.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.4.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.4.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.4.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.4.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.4.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.4.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.5.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.5.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.5.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.5.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.5.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.5.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.5.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.6.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.6.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.6.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.6.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.6.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.6.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.6.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.7.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.7.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.7.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.7.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.7.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.7.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.7.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.8.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.8.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.8.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.8.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.8.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.8.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.8.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.9.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.9.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.9.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.9.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.9.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.9.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.9.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.10.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.10.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.10.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.10.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.10.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.10.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.10.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.11.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.11.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.11.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.11.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.11.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.11.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.11.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.12.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.12.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.12.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.12.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.12.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.12.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.12.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.13.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.13.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.13.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.13.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.13.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.13.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.13.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.14.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.14.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.14.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.14.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.14.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.14.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.14.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.15.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.15.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.15.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.15.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.15.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.15.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.15.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.16.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.16.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.16.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.16.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.16.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.16.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.16.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.17.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.17.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.17.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.17.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.17.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.17.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.17.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.18.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.18.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.18.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.18.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.18.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.18.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.18.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.19.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.19.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.19.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.19.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.19.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.19.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.19.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.20.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.20.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.20.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.20.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.20.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.20.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.20.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.21.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.21.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.21.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.21.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.21.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.21.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.21.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.22.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.22.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.22.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.22.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.22.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.22.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.22.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.22.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.23.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.23.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.23.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.23.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.23.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.23.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.23.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.23.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.24.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.24.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.24.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.24.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.24.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.24.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.24.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.24.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.25.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.25.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.25.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.25.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.25.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.25.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.25.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.25.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.26.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.26.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.26.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.26.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.26.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.26.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.26.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.26.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.self_attn.q_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.27.self_attn.q_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.self_attn.q_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.self_attn.k_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.27.self_attn.k_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.self_attn.k_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.self_attn.v_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.27.self_attn.v_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.self_attn.v_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.self_attn.o_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.27.self_attn.o_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.self_attn.o_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.mlp.gate_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.27.mlp.gate_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.mlp.gate_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.mlp.up_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.27.mlp.up_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.mlp.up_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.mlp.down_proj.base_layer <class 'bitsandbytes.nn.modules.Linear4bit'>\n",
      "model.layers.27.mlp.down_proj.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.27.mlp.down_proj.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n",
      "lm_head.base_layer <class 'torch.nn.modules.linear.Linear'>\n",
      "lm_head.lora_A.default <class 'torch.nn.modules.linear.Linear'>\n",
      "lm_head.lora_B.default <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        print(name, type(module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokenizer, train_dataset, val_dataset, output_dir):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Qwen modellerinde genellikle Linear modüllerin isimleri farklı olabilir. find_all_linear_names() ile kontrol et.\n",
    "    target_modules = [\"w1\", \"w2\", \"w3\", \"out_proj\", \"lm_head\"] # Target modules for qwen\n",
    "    print(\"Target modules for LoRA:\", target_modules)\n",
    "\n",
    "    peft_config = create_peft_config(target_modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable params: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=4,\n",
    "            num_train_epochs=3,\n",
    "            gradient_accumulation_steps=4,\n",
    "            learning_rate=2e-4,\n",
    "            bf16=True,\n",
    "            fp16=False,\n",
    "            logging_steps=1,\n",
    "            eval_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=5,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            report_to=None\n",
    "        ),\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634c38c6abb2461cb2bffa87586c57b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl:   0%|          | 0.00/68.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa6da14ab4d43c295d7a5ed194a0334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl:   0%|          | 0.00/12.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beaee81a8eb543c99e1ce9460271f84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0a9f8d371e41848beb7b94876a0330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': [0, 1], 'Gene1': ['KinC', 'KinD'], 'Gene2': ['Spo0A~P', 'Spo0A~P'], 'Keywords': ['responsible, production', 'responsible, production'], 'Direct dependency relations': ['nmod(responsible, production)', 'nmod(responsible, production)'], 'INO Type': ['regulation of translation', 'regulation of translation'], 'INO ID': ['INO_0000034', 'INO_0000034'], 'Num. Keywords': ['2', '2'], 'Sentence': ['In vivo studies of the activity of four of the kinases, KinA, [PROTEIN1] KinC [/PROTEIN1], KinD (ykvD) and KinE (ykrQ), using abrB transcription as an indicator of [PROTEIN2] Spo0A~P [/PROTEIN2] level, revealed that KinC and KinD were responsible for Spo0A~P production during the exponential phase of growth in the absence of KinA and KinB.', 'In vivo studies of the activity of four of the kinases, KinA, KinC, [PROTEIN1] KinD [/PROTEIN1] (ykvD) and KinE (ykrQ), using abrB transcription as an indicator of [PROTEIN2] Spo0A~P [/PROTEIN2] level, revealed that KinC and KinD were responsible for Spo0A~P production during the exponential phase of growth in the absence of KinA and KinB.']}\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data_files = {\n",
    "    \"train\": \"train.jsonl\",\n",
    "    \"test\": \"test.jsonl\"\n",
    "}\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bengisucam/LLL_INO-tagged\", data_files=data_files, cache_dir=None, download_mode='force_redownload')\n",
    "\n",
    "# Access the splits directly from the loaded dataset\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "\n",
    "print(train_dataset[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N GPUS: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e6fddb92294236b4b49ad5a8cc42df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/825 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d84c00d8c394c2fbb4935746279d943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df86d7cb773a44e5a7b55e82a802c642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aff6bfc51c14c5c8bbeb63501a2db11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ce1673d540471bbb80cc066e5cdb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30c7ae3dc2148e1822a1c7e044ffe12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf64f6f1a3c41098069eb2301eb6aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74440aec3158440db2a9079755543784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb910a82d4d468fb525eba00edc1d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfa077a60af4eab8c2bfabc96f0435f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408ad9cd9ca244fa8aebb6b846e5f3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a78ac3b6b140e4bce79603676afc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe434e773ff34b0eb19c414e028e24dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Qwen 2.5 7B 1M Token Instruct Model\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct-1M\"\n",
    "bnb_config = create_bnb_config()\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a742f8235e45b894795afc55f96790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prompt:\n",
      " <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "In vivo studies of the activity of four of the kinases, KinA, [PROTEIN1] KinC [/PROTEIN1], KinD (ykvD) and KinE (ykrQ), using abrB transcription as an indicator of [PROTEIN2] Spo0A~P [/PROTEIN2] level, revealed that KinC and KinD were responsible for Spo0A~P production during the exponential phase of growth in the absence of KinA and KinB.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "responsible, production<|im_end|>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9338856834647fa942d5c7657447f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf4e9bb8dbb4d54969d008ae6fd498a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976f57d8659d46728e27912471a53aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prompt:\n",
      " <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "In addition to the typical [PROTEIN1] sigmaB [/PROTEIN1]-dependent, stress- and starvation-inducible pattern, [PROTEIN2] yvyD [/PROTEIN2] is also induced in response to amino acid depletion.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "dependent, inducible<|im_end|>\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd687541d294cb98e4c68f481424537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8d5ccf9e5c4c36b717a7a514610ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "max_length = 1024\n",
    "train_dataset_processed = preprocess_dataset(tokenizer, max_length, seed, train_dataset)\n",
    "val_dataset_processed = preprocess_dataset(tokenizer, max_length, seed, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('self', 'output_dir', 'overwrite_output_dir', 'do_train', 'do_eval', 'do_predict', 'eval_strategy', 'prediction_loss_only', 'per_device_train_batch_size', 'per_device_eval_batch_size', 'per_gpu_train_batch_size', 'per_gpu_eval_batch_size', 'gradient_accumulation_steps', 'eval_accumulation_steps', 'eval_delay', 'torch_empty_cache_steps', 'learning_rate', 'weight_decay', 'adam_beta1', 'adam_beta2', 'adam_epsilon', 'max_grad_norm', 'num_train_epochs', 'max_steps', 'lr_scheduler_type', 'lr_scheduler_kwargs', 'warmup_ratio', 'warmup_steps', 'log_level', 'log_level_replica', 'log_on_each_node', 'logging_dir', 'logging_strategy', 'logging_first_step', 'logging_steps', 'logging_nan_inf_filter', 'save_strategy', 'save_steps', 'save_total_limit', 'save_safetensors', 'save_on_each_node', 'save_only_model', 'restore_callback_states_from_checkpoint', 'no_cuda', 'use_cpu', 'use_mps_device', 'seed', 'data_seed', 'jit_mode_eval', 'use_ipex', 'bf16', 'fp16', 'fp16_opt_level', 'half_precision_backend', 'bf16_full_eval', 'fp16_full_eval', 'tf32', 'local_rank', 'ddp_backend', 'tpu_num_cores', 'tpu_metrics_debug', 'debug', 'dataloader_drop_last', 'eval_steps', 'dataloader_num_workers', 'dataloader_prefetch_factor', 'past_index', 'run_name', 'disable_tqdm', 'remove_unused_columns', 'label_names', 'load_best_model_at_end', 'metric_for_best_model', 'greater_is_better', 'ignore_data_skip', 'fsdp', 'fsdp_min_num_params', 'fsdp_config', 'fsdp_transformer_layer_cls_to_wrap', 'accelerator_config', 'deepspeed', 'label_smoothing_factor', 'optim', 'optim_args', 'adafactor', 'group_by_length', 'length_column_name', 'report_to', 'ddp_find_unused_parameters', 'ddp_bucket_cap_mb', 'ddp_broadcast_buffers', 'dataloader_pin_memory', 'dataloader_persistent_workers', 'skip_memory_metrics', 'use_legacy_prediction_loop', 'push_to_hub', 'resume_from_checkpoint', 'hub_model_id', 'hub_strategy', 'hub_token', 'hub_private_repo', 'hub_always_push', 'gradient_checkpointing', 'gradient_checkpointing_kwargs', 'include_inputs_for_metrics', 'include_for_metrics', 'eval_do_concat_batches', 'fp16_backend', 'push_to_hub_model_id', 'push_to_hub_organization', 'push_to_hub_token', 'mp_parameters', 'auto_find_batch_size', 'full_determinism', 'torchdynamo', 'ray_scope', 'ddp_timeout', 'torch_compile', 'torch_compile_backend', 'torch_compile_mode', 'include_tokens_per_second', 'include_num_input_tokens_seen', 'neftune_noise_alpha', 'optim_target_modules', 'batch_eval_metrics', 'eval_on_start', 'use_liger_kernel', 'eval_use_gather_object', 'average_tokens_across_devices')\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "print(TrainingArguments.__init__.__code__.co_varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target modules for LoRA: ['w1', 'w2', 'w3', 'out_proj', 'lm_head']\n",
      "Trainable params: 21,487,616 / 4,374,459,904 (0.49%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbusra-oguzoglu\u001b[0m (\u001b[33mbusra-ai-test\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250615_090835-qcryyolf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/busra-ai-test/huggingface/runs/qcryyolf' target=\"_blank\">results/qwen2.5_colab/</a></strong> to <a href='https://wandb.ai/busra-ai-test/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/busra-ai-test/huggingface' target=\"_blank\">https://wandb.ai/busra-ai-test/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/busra-ai-test/huggingface/runs/qcryyolf' target=\"_blank\">https://wandb.ai/busra-ai-test/huggingface/runs/qcryyolf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 01:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.734700</td>\n",
       "      <td>1.441218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.003500</td>\n",
       "      <td>0.932895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.792400</td>\n",
       "      <td>0.801653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.569300</td>\n",
       "      <td>0.732333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.627500</td>\n",
       "      <td>0.695054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "output_dir = \"results/qwen2.5_colab/\"\n",
    "train(model, tokenizer, train_dataset_processed, val_dataset_processed, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import  LlamaTokenizer, set_seed\n",
    "from peft import  AutoPeftModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime as dt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "# Prompt oluşturucu\n",
    "def create_prompt_formats_for_test(sample):\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\"\n",
    "    INPUT_KEY = \"### Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    sample[\"text\"] = \"\\n\\n\".join([\n",
    "        INTRO_BLURB,\n",
    "        INSTRUCTION_KEY,\n",
    "        f\"{INPUT_KEY}\\n{sample['Sentence']}\",\n",
    "        f\"{RESPONSE_KEY}\\n\"\n",
    "    ])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen2.5 model checkpoint with minimal memory...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f695e37981214bb184225500bc814fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8976e65c0495429881260f402d700375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 1 | Sentence ID: 135\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "In addition to the typical [PROTEIN1] sigmaB [/PROTEIN1]-dependent, stress- and starvation-inducible pattern, [PROTEIN2] yvyD [/PROTEIN2] is also induced in response to amino acid depletion.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins [Protein1] (sigmaB) and [Protein2] (yvyD) in the given sentence is \"induced.\" This indicates that both proteins are activated or expressed in response\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 2 | Sentence ID: 115\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "We overproduced and purified sigma(X) from Escherichia coli and demonstrate that in vitro, both sigma(A) and [PROTEIN1] sigma(X) [/PROTEIN1] holoenzymes recognize promoter elements within the sigX-[PROTEIN2] ypuN [/PROTEIN2] control region.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins tagged with [Protein1] (sigma(X)) and [Protein2] (ypuN) in the given sentence is \"recognize.\"\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 3 | Sentence ID: 131\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "The sigmaB-dependent promoter drives expression of yvyD under stress conditions and after glucose starvation, whereas a [PROTEIN1] sigmaH [/PROTEIN1]-dependent promoter is responsible for [PROTEIN2] yvyD [/PROTEIN2] transcription following amino acid limitation.\n",
      "\n",
      "### Response:\n",
      "To identify the key word representing the interaction between the proteins [Protein1] (sigmaH) and [Protein2] (yvyD) in the given sentence, we need to focus on the part of the sentence that describes their relationship\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 4 | Sentence ID: 55\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "Three new [PROTEIN1] sigmaB [/PROTEIN1]-dependent genes (ydaE, [PROTEIN2] ydaG [/PROTEIN2] and yfkM) encoding proteins with still unknown functions were also described.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins tagged with [Protein1] and [Protein2] in the given sentence is \"sigmaB\". \n",
      "\n",
      "However, it's important to note that \"sigmaB\" does not directly represent an\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 5 | Sentence ID: 95\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "Transcription of the cotB, cotC, and [PROTEIN2] cotX [/PROTEIN2] genes by final [PROTEIN1] sigma(K) [/PROTEIN1] RNA polymerase is activated by a small, DNA-binding protein called GerE.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins tagged with [Protein1] and [Protein2] in the given sentence is \"activated.\"\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 6 | Sentence ID: 29\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "The [PROTEIN2] rocG [/PROTEIN2] gene of Bacillus subtilis, encoding a catabolic glutamate dehydrogenase, is transcribed by SigL-containing RNA polymerase and requires for its expression [PROTEIN1] RocR [/PROTEIN1], a member of the NtrC/NifA family of proteins that bind to enhancer-like elements, called upstream activating sequences (UAS).\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins [Protein1] and [Protein2] in the given sentence is \"requires.\"\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 7 | Sentence ID: 157\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "The [PROTEIN1] sigma W [/PROTEIN1] regulon includes a penicillin binding protein ([PROTEIN2] PBP4* [/PROTEIN2]) and a co-transcribed amino acid racemase (RacX), homologues of signal peptide peptidase (YteI), flotillin (YuaG), ABC transporters (YknXYZ), non-haem bromoperoxidase (YdjP), epoxide hydrolase (YfhM) and three small peptides with structural similarities to bacteriocin precursor polypeptides.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins tagged with [Protein1] and [Protein2] in the given sentence is \"includes.\"\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 8 | Sentence ID: 51\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "Dephosphorylation of [PROTEIN2] SpoIIAA-P [/PROTEIN2] by [PROTEIN1] SpoIIE [/PROTEIN1] is strictly dependent on the presence of the bivalent metal ions Mn2+ or Mg2+.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins [Protein1] SpoIIE [/Protein1] and [Protein2] SpoIIAA-P [/Protein2] in the given sentence is \"dephosphorylation\".\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 9 | Sentence ID: 101\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "In vitro transcription experiments suggest that the differential pattern of [PROTEIN2] cot [/PROTEIN2] gene expression results from the combined action of GerE and another transcription factor, [PROTEIN1] SpoIIID [/PROTEIN1].\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins tagged with [Protein1] and [Protein2] in the given sentence is \"cot\".\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 10 | Sentence ID: 145\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "The class includes AsiA form bacteriophage T4, which inhibits Escherichia coli sigma 70; FlgM, present in both gram-positive and gram-negative bacteria, which inhibits the flagella sigma factor sigma 28; [PROTEIN1] SpoIIAB [/PROTEIN1], which inhibits the sporulation-specific sigma factor,[PROTEIN2] sigma F [/PROTEIN2]and sigma G, of Bacillus subtilis; RbsW of B. subtilis, which inhibits stress response sigma factor sigma B; and DnaK, a general regulator of the heat shock response, which in bacteria inhibits the heat shock sigma factor sigma 32.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins [PROTEIN1] SpoIIAB [/PROTEIN1] and [PROTEIN2] sigma F [/PROTEIN2] in the given sentence is \"inhibits.\"\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 11 | Sentence ID: 19\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "Therefore, the physiological role of [PROTEIN1] sigmaB [/PROTEIN1]-dependent [PROTEIN2] katX [/PROTEIN2] expression remains obscure.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins [Protein1] sigmaB and [Protein2] katX in the given sentence is \"dependent\".\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 12 | Sentence ID: 85\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "The expression of the [PROTEIN2] bkd [/PROTEIN2] operon was induced by the presence of isoleucine or valine in the growth medium and depended upon the presence of the sigma factor [PROTEIN1] SigL [/PROTEIN1], a member of the sigma 54 family.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence is \"depended\".\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 13 | Sentence ID: 15\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "The expression of [PROTEIN2] rsfA [/PROTEIN2] is under the control of both sigma(F) and [PROTEIN1] sigma(G) [/PROTEIN1].\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins tagged with [Protein1] and [Protein2] in the given sentence is \"under.\"\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 14 | Sentence ID: 66\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "A low level of GerE activated transcription of cotD by sigmaK RNA polymerase in vitro, but a higher level of [PROTEIN1] GerE [/PROTEIN1] repressed [PROTEIN2] cotD [/PROTEIN2] transcription.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins [Protein1] and [Protein2] in the given sentence is \"repressed.\"\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 15 | Sentence ID: 24\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "[PROTEIN2] sigmaF [/PROTEIN2] is controlled by a regulatory cascade involving an anti-sigma factor, [PROTEIN1] SpoIIAB [/PROTEIN1], an anti-anti-sigma factor, SpoIIAA, and a membrane-bound phosphatase, SpoIIE, which converts the inactive, phosphorylated form of SpoIIAA back to the active form.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins [Protein1] SpoIIAB [/Protein1] and [Protein2] sigmaF [/Protein2] in the given sentence is \"controlled.\" The sentence states that sigmaF\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 16 | Sentence ID: 30\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "Unlike the case for other [PROTEIN1] sigma(54) [/PROTEIN1]-dependent genes, [PROTEIN2] rocG [/PROTEIN2] has no UAS; instead, its expression depends on a sequence located 1.5 kilobases downstream of the rocG promoter, beyond the end of the rocG coding region.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins [Protein1] and [Protein2] in the given sentence is \"depends.\" However, it's important to note that the sentence does not explicitly describe an interaction between [Protein1\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 17 | Sentence ID: 132\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "This indicates that the [PROTEIN2] yvyD [/PROTEIN2] gene product, being a member of both the [PROTEIN1] sigmaB [/PROTEIN1] and sigmaH regulons, might negatively regulate the activity of the sigmaL regulon.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins tagged with [Protein1] and [Protein2] in the given sentence is \"regulons\".\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 18 | Sentence ID: 105\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "A low concentration of [PROTEIN1] GerE [/PROTEIN1] activated cotB transcription by final sigma(K) RNA polymerase, whereas a higher concentration was needed to activate transcription of cotX or [PROTEIN2] cotC [/PROTEIN2].\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins tagged with [Protein1] (GerE) and [Protein2] (cotC) in the given sentence is \"activated\".\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 19 | Sentence ID: 152\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "The sspH and [PROTEIN2] sspL [/PROTEIN2] genes are recognized primarily by the forespore-specific sigma factor for RNA polymerase, [PROTEIN1] sigmaG [/PROTEIN1]; the sspN-tlp operon is recognized equally well by sigmaG and the other forespore-specific sigma factor, sigmaF.\n",
      "\n",
      "### Response:\n",
      "To determine the key word representing the interaction between the proteins tagged with [Protein1] (sigmaG) and [Protein2] (sspL), we need to identify the term that indicates their functional relationship or interaction within the context of the\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 20 | Sentence ID: 16\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "Expression of the [PROTEIN1] sigma(K) [PROTEIN1]-dependent [PROTEIN2] cwlH [/PROTEIN2] gene depended on gerE.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins tagged with [Protein1] and [Protein2] in the given sentence is \"dependent\".\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 21 | Sentence ID: 75\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "[PROTEIN1] ComK [/PROTEIN1] negatively controls the transcription of hag by stimulating the transcription of comF-[PROTEIN2] flgM [/PROTEIN2], thereby increasing the production of the FlgM antisigma factor that inhibits sigmaD activity.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins tagged with [Protein1] (ComK) and [Protein2] (FlgM) in the given sentence is \"stimulating.\" This indicates that ComK interacts with Flg\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 22 | Sentence ID: 18\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "Our results indicate that the level of [PROTEIN2] KatX [/PROTEIN2] level in outgrowing spores depends mainly on [PROTEIN1] EsigmaF [/PROTEIN1], because sigB mutants show normal KatX activity in dormant and outgrowing spores.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins [Protein1] (EsigmaF) and [Protein2] (KatX) in the given sentence is \"depends.\" This indicates that the level of KatX is influenced by Es\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 23 | Sentence ID: 12\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "In the shaA mutant, [PROTEIN1] sigma(H) [/PROTEIN1]-dependent expression of [PROTEIN2] spo0A [/PROTEIN2] and spoVG at an early stage of sporulation was sensitive to external NaCl.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins [Protein1] (sigma(H)) and [Protein2] (spo0A and spoVG) in the given sentence is \"dependent.\" This indicates that the expression of spo0A\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 24 | Sentence ID: 9\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "Analysis of the expression of a translational ywhE-lacZ fusion showed that [PROTEIN2] ywhE [PROTEIN2] expression is sporulation-specific, and is controlled predominantly by the forespore-specific sigma factor sigma(F), and to a lesser extent by [PROTEIN1] sigma(G) [/PROTEIN1].\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins tagged with [Protein1] and [Protein2] in the given sentence is \"controlled.\"\n",
      "###############################\n",
      "EXAMPLE 25 | Sentence ID: 31\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction: What is the key word that represents the interaction between the proteins which are tagged with [Protein1] and [Protein2] in the given sentence?\n",
      "\n",
      "### Input:\n",
      "In addition to controlling transcription of [PROTEIN2] phrC [/PROTEIN2], [PROTEIN1] sigmaH [/PROTEIN1] appears to control expression of at least one other gene required for production of CSF.\n",
      "\n",
      "### Response:\n",
      "The key word that represents the interaction between the proteins [Protein1] (sigmaH) and [Protein2] (phrC) in the given sentence is \"controls.\"\n",
      "###############################\n"
     ]
    }
   ],
   "source": [
    "# Log ayarları\n",
    "logging.basicConfig(filename=\"finetune_results/qwen2.5-light-infer.log\", level=logging.INFO)\n",
    "logging.info(f\"({dt.now().strftime('%d/%m/%Y %H:%M:%S')}) | START\")\n",
    "\n",
    "# Cihaz tanımı\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model yolu\n",
    "tmp_model_path = \"results/qwen2.5_colab/\"\n",
    "print(\"Loading Qwen2.5 model checkpoint with minimal memory...\")\n",
    "\n",
    "# Model yükle\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    tmp_model_path,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model.eval()  # inference modu\n",
    "model.to(device)\n",
    "\n",
    "# Tokenizer yükle\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tmp_model_path,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=False\n",
    ")\n",
    "\n",
    "# Dataset\n",
    "test_dataset = load_dataset(\"bengisucam/LLL_INO-tagged\", split=\"test\")\n",
    "test_dataset = test_dataset.map(create_prompt_formats_for_test)\n",
    "print(\"Dataset size:\", len(test_dataset))\n",
    "\n",
    "# Inference\n",
    "for i in range(len(test_dataset)):\n",
    "    sample = test_dataset[i]\n",
    "    text = sample[\"text\"]\n",
    "    sentence_id = sample[\"Unnamed: 0\"]\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=50,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    # Çıktıyı çöz\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = decoded.split(\"[/INST]\")[-1].strip() if \"[/INST]\" in decoded else decoded\n",
    "\n",
    "    print(f\"EXAMPLE {i+1} | Sentence ID: {sentence_id}\")\n",
    "    print(response)\n",
    "    print(\"###############################\")\n",
    "\n",
    "    logging.info(\"Sentence Id: %s | Response: %s\", sentence_id, response)\n",
    "\n",
    "    # GPU belleğini boşalt\n",
    "    del inputs, outputs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12310"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements-qwen2.5_instructv3.txt"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
